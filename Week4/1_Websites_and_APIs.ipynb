{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cfb95c-370a-408b-a519-d72b2c243a75",
   "metadata": {},
   "source": [
    "# Websites and APIs\n",
    "**Web scraping** and **APIs** are popular methods for collecting data from websites. \n",
    "\n",
    "## Webscraping\n",
    "Webscraping involves directly parsing a website's HTML. This method can extract a wide range of data available on a page. It can also introduce complexity to your project, especially if the website's data is not consistently structured. \n",
    "\n",
    "## API (Application Programming Interface)\n",
    "APIs provide structured data and detailed documentation for querying the website and filtering results. They are generally easier to use but may come with restrictions, such as limits on the number of requests per day or the number of records you can retrieve. \n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Copyright | Terms of Use\n",
    "Before starting a webscraping or API project, you must \n",
    "\n",
    "## Review and understand the terms of use.\n",
    "\n",
    "o\tDo the terms of service include any restrictions or guidelines?\n",
    "\n",
    "o\tAre permissions/licenses needed to scrape data? If yes, have you obtained these permissions/licenses?\n",
    "\n",
    "o\tIs the information publicly available?\n",
    "\n",
    "o\tIf a database, is the database protected by copyright? Or in the public domain?\n",
    "\n",
    "## Fair Use \n",
    "Limited use of copyrighted materials is allowed under certain conditions for journalism, scholarship, and teaching. [Use the Resources for determining fair use](https://library.osu.edu/copyright/fair-use) to verify your project is within the scope of fair use. Contact University Libraries [Copyright Services](https://library.osu.edu/copyright) if you have any questions.\n",
    "\n",
    "## Check for robots.txt directives\n",
    "robots.txt directives limit web-scraping or web-crawling. Respect these directives.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27b9f94-8e37-49a7-8f14-94b673599f5d",
   "metadata": {},
   "source": [
    "# Step 2: Is an API available?\n",
    "APIs can simplify data collection from a website by returning structured data (e.g. JSON, XML) Examples of APIs include:\n",
    "\n",
    "- [PubMed eUtilities](https://www.ncbi.nlm.nih.gov/books/NBK25500/)\n",
    "- [Elsevier APIs](https://dev.elsevier.com/)\n",
    "- [Spotify Web API](https://developer.spotify.com/documentation/web-api)\n",
    "\n",
    "To determine is an API is available, try searching for the name of the website and \"API\" or \"documentation.\" If an API is available, read the terms of use and consider factors like rate limits, costs, and access restrictions. \n",
    "\n",
    "If an API is not available, that's okay. Data collection might be a bit more complex, but remember to respect copyright and terms of use.\n",
    "\n",
    "## Activity 1: The Lantern - Part 1\n",
    "Go to the [OSU Publication Archives website](https://osupublicationarchives.osu.edu/?a=p&p=home&e=-------en-20--1--txt-txIN-------). **Note** that The Ohio State University provides the online archives of Ohio State's student newspaper *The Lantern*, the student yearbook *The Makio*, and alumni magazines for research and educational purposes. This is explicitly stated at the bottom of this page.\n",
    "\n",
    "Search for `homecoming parade`. On the left, adjust the filters to show results for the decade `1970-1979`, publication `The Lantern`, and category `Article`. \n",
    "\n",
    "![TheLantern_HomecomingParade.png](images/TheLantern_HomecomingParade.png \"Screenshot of search result\")\n",
    "\n",
    "Once you've set your filters, you should have 51 results. The first 20 search results are displayed on page 1.\n",
    "\n",
    "Take a look at the search URL.\n",
    "\n",
    "![search_results_1.png](images/search_results_1.png \"Screenshot of url, r=1\")\n",
    "\n",
    "Scroll to the bottom and click on page 2 to see search results 21-40. Note that the search result has changed to\n",
    "\n",
    "![search_results_2.png](images/search_results_2.png \"Screenshot of url, r now equals 21\")\n",
    "\n",
    "Return to page 1 of your search, scroll to the right end of your search URL and add the characters `&f=XML` to the end of the string.\n",
    "\n",
    "![search_results_3.png](images/search_results_3.png \"Screenshot of url with &f=XML added to the end of the url\")\n",
    "\n",
    "As you've searched for `homecoming parade`, the decade `1970-1979`, publication `The Lantern` and category  `Article`, the software powering the osupublications service has constructed a server request and inserted your parameters into the url. By adding `&f=XML` to the request parameters, the server returns structured XML output. We can iterate through this output to gather our data.\n",
    "\n",
    "### Search parameters\n",
    "#### The Lantern\n",
    "![parameter_publication_the_lantern.png](images/parameter_publication_the_lantern.png \"Screenshot showing position of publication identifier in search url\")\n",
    "\n",
    "#### homecoming+parade\n",
    "![parameter_homecoming_parade.png](images/parameter_homecoming_parade.png \"Screenshot showing position of homecoming parade in search url\")\n",
    "\n",
    "#### Decade: 1970-1979\n",
    "![parameter_decade.png](images/parameter_decade.png \"Screenshot showing position of decade filter in search url\")\n",
    "\n",
    "#### Category: Article\n",
    "![parameter_category_article.png](images/parameter_category_article.png \"Screenshot showing position of category filter in search url\")\n",
    "\n",
    "#### XML\n",
    "![search_results_3.png](images/search_results_3.png \"Screenshot of url with &f=XML added to the end of the url\")\n",
    "\n",
    "![TheLantern_XML.png](images/TheLantern_XML.png \"Screenshot of XML output\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad70926-6af8-4a2c-bbcc-62371ca1d9e4",
   "metadata": {},
   "source": [
    "# Step 3. Inspect the elements\n",
    "\n",
    "XML and HTML are tree-structured documents. When you request a search URL, it retrieves an HTML or XML page from a server. The browser then downloads the page into local memory and parses the HTML or XML for display.\n",
    "\n",
    "The [Document Object Model (DOM)](https://en.wikipedia.org/wiki/Document_Object_Model) respresents the overall tree-structure of the XML or HTML document. For example, in the XML document shown in Step2:\n",
    "- `VeridianXMLResponse` represents the document node.\n",
    "- All XML elements within `VeridianXMLResponse` are element nodes. \n",
    "- There is some HTML present in the `SearchResultSnippetHTML` node.\n",
    "- There are no XML attribute nodes, but there are HTML attribute nodes in the `SearchResultSnippetHTML` node.\n",
    "- Text between the XML elements are text nodes.\n",
    "\n",
    "The tree is hierachically structured and each tree branch ends with a node. Each node contains objects, and nodes can be nested within nodes.\n",
    "\n",
    "## Developer Tools\n",
    "\n",
    "The XML returned for our `The Lantern` search is well structured.\n",
    "\n",
    "- A unique identifier is provided for each article under the tag `<LogicalSectionID>`\n",
    "- The title for each article appears in the tag `<LogicalSectionTitle>`\n",
    "- The category type is included in the tag `<LogicalSectionType>`\n",
    "\n",
    "HTML responses are often less clear but can be navigated with persistence. Google offers a range of [Developer Tools](https://developer.chrome.com/docs/devtools/dom) that can help understand a webpage's DOM elements. For example, if we search the [OSU Publication Archives](https://osupublicationarchives.osu.edu/?a=p&p=home&e=-------en-20--1--txt-txIN-------) website for `homecoming parade`, decade `1970-1979`, publication `The Lantern`, and category `Article` (removing `&f=XML` from the end of your search URL), our browser renders our search results in HTML. \n",
    "\n",
    "Right-click on the HTML search results and select \"Inspect\" to explore the DOM elements.\n",
    "\n",
    "![TheLantern_Inspect.png](attachment:f225f96f-9563-43da-b3bc-632c0410e7dd.png \"Screenshot showing the word Inspect at the bottom of options in window that opens after right clicking on search results\")\n",
    "\n",
    "This action opens Google's Developer Tools on the right side of your screen. **Note** The default tab, \"Elements,\" displays the HTML elements for the page. \n",
    "\n",
    "Click the inspect icon ![inspect_icon.png](images/inspect_icon.png \"Decorative\") and then select an HTML element on your page. ![TheLantern_Element.png](images/TheLantern_Element.png \"Screenshot showing selected element The Lantern 23 October 1978\")\n",
    "\n",
    "The Developer Tools will highlight where the element is located in the HTML and provide a tooltip with additional information about the element.  \n",
    "\n",
    "![TheLantern_DeveloperTools.png](images/TheLantern_DeveloperTools.png \"Screenshot showing the selected element is highlighted, the location of selected element is highlighted in the Developer Tools and a tooltip now appears above the selected element in the HTML\")\n",
    "\n",
    "To view more details about the element's location in the DOM structure, right-click on the element in the Developer Tools, select **Copy > Copy element**, and then paste the text into Notepad or a similar text editor.\n",
    "\n",
    "![TheLantern_CopyElement.png](images/TheLantern_CopyElement.png \"Screenshot showing the div tag is selected for the element The Lantern 23 October 1978, the menu that appears after right clicking on the element, and the then location of Copy and Copy element in the menu\")\n",
    "\n",
    "![TheLantern_Notepad.png](images/TheLantern_Notepad.png \"Screenshot of HTML snippet for selected element\")\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46217ce-30ed-48ee-9b18-5575c46fd801",
   "metadata": {},
   "source": [
    "## Activity 2: [Meet the Animals](https://nationalzoo.si.edu/animals/list) - Part 1\n",
    "\n",
    "[Meet the Animals](https://nationalzoo.si.edu/animals/list) at the Smithsonian National Zoo & Conservation Biology Institute. Practice using Google's Develop Tools to find the following elements for one animal. Copy the elements you find to Notepad.\n",
    "\n",
    "- Common name\n",
    "- Scientific name\n",
    "- Taxonomic information\n",
    "     - Class\n",
    "     - Order\n",
    "     - Family\n",
    "     - Genus and species\n",
    "- Physical description\n",
    "- Size\n",
    "- Native habitat\n",
    "- Conservation status\n",
    "- Fun facts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ba812-094d-4e32-bf94-732f983930f4",
   "metadata": {},
   "source": [
    "# Step 4: Identify Python libraries for project\n",
    "To gather XML and HTML data from websites and APIs, you'll need several Python libraries. Some libraries handle web server requests and responses, while others parse the retrieved content. Libraries like Pandas and CSV are used to store and output results as .csv files.\n",
    "\n",
    "## [requests](https://requests.readthedocs.io/en/latest/)\n",
    "The [requests](https://requests.readthedocs.io/en/latest/) library retrieves HTML or XML documents from a server and processes the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d9c73-b006-408c-93f8-49920a9db384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"INSERT URL HERE\"\n",
    "response=requests.get(url)\n",
    "text=response.text # This returns the response content as text\n",
    "bytes=response.content  # This returns the response content as bytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688741e6-6749-4515-9cb0-8a85cdf54c8e",
   "metadata": {},
   "source": [
    "## [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
    "\n",
    "[BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) parses HTML and XML documents, helping you search for and extract elements from the DOM. The first argument is the content to be parsed, and the second specifies the parsing library to use.\n",
    "\n",
    "- `html.parser` The default HTML parser.\n",
    "- `lxml` a faster parser with more features.* \n",
    "- `xml` parses XML\n",
    "- `html5lib` for HTML5 parsing.*\n",
    "\n",
    "Additional keyword arguments (**kwargs) are available. See the [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) documentation for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad54414-c2e7-425c-919a-ea5796b08b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"INSERT URL HERE\"\n",
    "response=requests.get(url).content\n",
    "soup=BeautifulSoup(response, 'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3031dfb-57eb-46a7-8064-8ad964ae6a19",
   "metadata": {},
   "source": [
    "Other Python libraries for parsing include:\n",
    "- [lxml.html](https://lxml.de/lxmlhtml.html)\n",
    "- [pyQuery](https://www.pyquery.org/pyquery-core-functions/)\n",
    "- [Selenium](https://www.selenium.dev/documentation/)\n",
    "\n",
    "Each library has its strengths and weaknesses. To learn more about different parsing tools read Anish Chapagain's [Hands-On Web Scraping with Python, 2nd edition](https://library.ohio-state.edu/record=b10892054~S7) \n",
    "\n",
    "*Verify that `lxml` or `html5lib` is installed in your Anaconda environment before using them.\n",
    "\n",
    "## [pandas](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "Pandas is a large Python library used for manipulating and analyzing tabular data. \n",
    "\n",
    "**Sample workflow**\n",
    "\n",
    "1. `import pandas as pd`\n",
    "2. Create a DataFrame to store results or data that you plan to gather. `DataFrame([data, index, columns, dtype, copy])`\n",
    "3. Gather variables\n",
    "4. Create a dictionary to store one row of variables.\n",
    "5. Create a DataFrame to store the one row of variables.\n",
    "6. Concatenate the DataFrame storing one row of variables to the DataFrame storing results that was constructed in step 2.\n",
    "7. Export results to .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5fe483-53a6-451f-8bc9-aa433afd5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results=pd.DataFrame(columns=['name','age','profession'])\n",
    "\n",
    "name=\"Jordan\"\n",
    "age=8\n",
    "profession=\"chipmunk control\"\n",
    "\n",
    "pets={\n",
    "    \"name\":name,\n",
    "    \"age\":age,\n",
    "    \"profession\":profession\n",
    "    }\n",
    "\n",
    "add_row_to_results=pd.DataFrame(pets, index=[0])\n",
    "results=pd.concat([add_row_to_results, results], axis=0, ignore_index=True)\n",
    "results.to_csv('pets2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e24418-e3cd-4ed3-b0c0-cccd5ec78a08",
   "metadata": {},
   "source": [
    "## [csv](https://docs.python.org/3/library/csv.html)\n",
    "The csv module both writes and reads .csv data.\n",
    "\n",
    "**Sample workflow**\n",
    "\n",
    "1. `import csv`\n",
    "2. Create an empty list named dataset\n",
    "3. Assign .csv headers to a list named columns\n",
    "4. Define the writeto_csv function to write results to a .csv file\n",
    "5. Gather variable\n",
    "6. For each row of data, append a list of variables following the order of the .csv headers to the dataset list.\n",
    "7. Use the writeto_csv function to write results to a .csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267123b9-a587-44f3-b1eb-59d8a19921d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "#####     STEP 1 - CREATE EMPTY DATASET AND DEFINE CSV HEADINGS     ##### \n",
    "dataSet=[]\n",
    "columns=['name','age','profession'] # for CSV headings\n",
    "\n",
    "\n",
    "#####     STEP 2 - DEFINE FUNCTION TO WRITE RESULTS TO CSV FILE     #####\n",
    "\n",
    "def writeto_csv(data,filename,columns):\n",
    "    with open(filename,'w+',newline='',encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file,fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])\n",
    "\n",
    "\n",
    "name=\"Jordan\"\n",
    "age=8\n",
    "profession=\"chipmunk control\"\n",
    "\n",
    "dataSet.append([name, age, profession])\n",
    "\n",
    "writeto_csv(dataSet,'pets.csv',columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3b49a-da60-4376-a7ad-191e6f252173",
   "metadata": {},
   "source": [
    "## [time.sleep( )](https://docs.python.org/3/library/time.html#time.sleep)\n",
    "Most APIs limit the number of records you can request per second. time.sleep() suspends your program for the specified number of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aeb4df-f3bc-457a-a745-f20ec7236a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454229be-1113-4227-8175-568f0f34f5cf",
   "metadata": {},
   "source": [
    "## [datetime](https://docs.python.org/3/library/datetime.html#datetime.date.today)\n",
    "It is good practice to include a `last_updated` column in any dataset you've created after gathering HTML or XML data. The datetime module can be used to identify the date you last ran your Python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bdbd3-341e-4dd1-885e-6dd636579eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "last_updated=today"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c3709dd-5214-4bf4-bd9d-63d6374beb74",
   "metadata": {},
   "source": [
    "# Step 5: Write code\n",
    "\n",
    "## [Spyder](https://docs.spyder-ide.org/current/index.html) \n",
    "[Spyder](https://docs.spyder-ide.org/current/index.html) is an interactive development environment (IDE) that offers quick feedback as you iteratively create your code. Designed by and for scientists, engineers, and data analysts, Spyder allows you to interactively write code, explore your data, and more.\n",
    "\n",
    "## Activity 3: The Lantern - Part 2\n",
    "Using Spyder and our XML search for [homecoming parade](https://osupublicationarchives.osu.edu/?a=q&r=1&results=1&tyq=ARTICLE&e=------197-en-20-LTN-1--txt-txIN-homecoming+parade------&f=XML) in The Lantern, gather the following elements:\n",
    "\n",
    "- unique_id\n",
    "- article_title\n",
    "- article_type\n",
    "\n",
    "Then use the unique_id to gather the following elements for each publication:\n",
    "\n",
    "- publication_date\n",
    "- publication_text\n",
    "\n",
    "Export the elements to a .csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee419e-957e-48bb-8ef9-9608d888c583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
